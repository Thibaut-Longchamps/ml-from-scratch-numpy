{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b1dd93",
   "metadata": {},
   "source": [
    "## **Linear Regression from scratch** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e6c43",
   "metadata": {},
   "source": [
    "### **Project Objective :** ###\n",
    "\n",
    "The goal of this project is to implement linear regression from scratch using gradient descent on a synthetic dataset. The purpose is to build a solid understanding of the linear algebra and matrix operations that underpin high-level machine learning libraries such as scikit-learn.\n",
    "\n",
    "This implementation includes key elements such as loss calculation (MSE, R²), parameter updates with gradient descent, and basic early stopping criteria.\n",
    "\n",
    "A deliberately small dataset is used to make the project more pedagogical and to clearly illustrate each computational step. This project is intended both as a learning exercise and as a foundation for extending towards more advanced models (e.g., logistic regression, softmax regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00160dd",
   "metadata": {},
   "source": [
    "Avant de réaliser une regression certain postulat statistiques essentielles sont à vérifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c0e44",
   "metadata": {},
   "source": [
    "### **Assumptions of linear regression**  ###\n",
    "\n",
    "- Linearity  \n",
    "- Independence of errors  \n",
    "- Homoscedasticity (constant variance of errors)  \n",
    "- Normality of errors (for inference)  \n",
    "- No multicollinearity  \n",
    "\n",
    "These are the classical assumptions behind linear regression. If you’re interested, I invite you to explore them in more detail, as they are key to understanding when the model is reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3a548",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "Before training any machine learning model, it is essential to perform an **exploratory data analysis (EDA)**.  \n",
    "This usually includes:\n",
    "- Looking at the distribution of the target variable  \n",
    "- Checking correlations or covariance between features  \n",
    "- Detecting multicollinearity  \n",
    "- Identifying missing values  \n",
    "- Visualizing relationships (scatter plots, box plots, histograms, etc.)  \n",
    "\n",
    "=> This step helps to understand the dataset, validate assumptions, and guide preprocessing choices.  \n",
    "\n",
    "=> In this notebook, since the data is **synthetic** and to keep things digestible, we skip the EDA.  \n",
    "But in a real project, **EDA is a crucial step** before building any predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea4883",
   "metadata": {},
   "source": [
    "### **1. Import the usual libraries** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fece89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085c23e",
   "metadata": {},
   "source": [
    "### **2. Import data** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "609cd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV\n",
    "df = pd.read_csv(r'C:\\Users\\33760\\Desktop\\entretiens_techniques\\data\\dataset_rent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "be84d75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surface</th>\n",
       "      <th>furnished</th>\n",
       "      <th>distance_metro_km</th>\n",
       "      <th>district</th>\n",
       "      <th>rent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>center</td>\n",
       "      <td>984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>suburb</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>suburb</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>periphery</td>\n",
       "      <td>2210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>center</td>\n",
       "      <td>1216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   surface  furnished  distance_metro_km   district  rent\n",
       "0       20          0                0.2     center   984\n",
       "1       35          1                0.1     suburb  1792\n",
       "2       50          0                0.8     suburb  1986\n",
       "3       45          1                0.5  periphery  2210\n",
       "4       28          0                0.3     center  1216"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43113baa",
   "metadata": {},
   "source": [
    "Columns of X:\n",
    "\n",
    "- surface (m²) → numerical\n",
    "\n",
    "- furnished (0/1) → already numerical (no encoding required)\n",
    "\n",
    "- distance_metro (km) → numerical\n",
    "\n",
    "- district = [\"center\", \"suburb\", \"outskirts\"]\n",
    "\n",
    "Column of y :\n",
    "\n",
    "- rent → target\n",
    "\n",
    "**Objective: predict the rent price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1af60852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 30 rows and 5 columns.\n"
     ]
    }
   ],
   "source": [
    "# Shape dataset\n",
    "rows, cols = df.shape\n",
    "\n",
    "print(f\"The dataset contains {rows} rows and {cols} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5fb8d",
   "metadata": {},
   "source": [
    "### **3. preprocessing before training** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a30375",
   "metadata": {},
   "source": [
    "##### 3.1 Convert DataFrame to numpy array #####\n",
    "\n",
    "Using NumPy allows us to perform the linear algebra calculations that are required for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f4c7dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data struture : <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe to numpy array\n",
    "data_arr = df.values\n",
    "\n",
    "print(f\"Data struture : {type(data_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0c4867d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 0, 0.2, 'center', 984],\n",
       "       [35, 1, 0.1, 'suburb', 1792],\n",
       "       [50, 0, 0.8, 'suburb', 1986],\n",
       "       [45, 1, 0.5, 'periphery', 2210],\n",
       "       [28, 0, 0.3, 'center', 1216],\n",
       "       [60, 1, 1.0, 'suburb', 2470],\n",
       "       [42, 0, 0.2, 'periphery', 1944],\n",
       "       [55, 1, 0.4, 'center', 2218],\n",
       "       [33, 0, 1.5, 'suburb', 1420],\n",
       "       [70, 1, 0.0, 'periphery', 3000],\n",
       "       [25, 0, 0.1, 'center', 1100],\n",
       "       [40, 1, 0.6, 'suburb', 1870],\n",
       "       [65, 0, 0.9, 'suburb', 2320],\n",
       "       [48, 1, 0.3, 'center', 2100],\n",
       "       [30, 0, 0.7, 'periphery', 1380],\n",
       "       [52, 1, 0.4, 'suburb', 2250],\n",
       "       [38, 0, 0.2, 'center', 1650],\n",
       "       [75, 1, 1.2, 'suburb', 3150],\n",
       "       [28, 1, 0.5, 'periphery', 1500],\n",
       "       [46, 0, 0.8, 'suburb', 1800],\n",
       "       [58, 1, 0.0, 'center', 2600],\n",
       "       [22, 0, 1.0, 'suburb', 1050],\n",
       "       [62, 1, 0.7, 'periphery', 2400],\n",
       "       [36, 0, 0.4, 'center', 1580],\n",
       "       [49, 1, 0.9, 'suburb', 2050],\n",
       "       [55, 0, 1.5, 'periphery', 1900],\n",
       "       [68, 1, 0.3, 'center', 2850],\n",
       "       [32, 0, 0.6, 'suburb', 1520],\n",
       "       [44, 1, 0.5, 'periphery', 2000],\n",
       "       [80, 1, 1.0, 'suburb', 3400]], dtype=object)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display array\n",
    "data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd644f6",
   "metadata": {},
   "source": [
    "##### 3.2 Separating explanatory variables (features) and the dependent variable (target) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d63cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Separate_X_y(data_arr):    \n",
    "    # Separate X, y\n",
    "    X = data_arr[:, :-1]\n",
    "    y = data_arr[:, -1]\n",
    "\n",
    "    # Dimension matrix X \n",
    "    n_samples = X.shape\n",
    "\n",
    "    # Check\n",
    "    print(f\"X : {X}\")\n",
    "    print(f\"shape X - rows : {n_samples[0]}, columns : {n_samples[1]}\")\n",
    "\n",
    "    print(f\"\\n y : {y}\")\n",
    "    print(f\"shape y - rows : {y.shape[0]}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b2878036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[20 0 0.2 'center']\n",
      " [35 1 0.1 'suburb']\n",
      " [50 0 0.8 'suburb']\n",
      " [45 1 0.5 'periphery']\n",
      " [28 0 0.3 'center']\n",
      " [60 1 1.0 'suburb']\n",
      " [42 0 0.2 'periphery']\n",
      " [55 1 0.4 'center']\n",
      " [33 0 1.5 'suburb']\n",
      " [70 1 0.0 'periphery']\n",
      " [25 0 0.1 'center']\n",
      " [40 1 0.6 'suburb']\n",
      " [65 0 0.9 'suburb']\n",
      " [48 1 0.3 'center']\n",
      " [30 0 0.7 'periphery']\n",
      " [52 1 0.4 'suburb']\n",
      " [38 0 0.2 'center']\n",
      " [75 1 1.2 'suburb']\n",
      " [28 1 0.5 'periphery']\n",
      " [46 0 0.8 'suburb']\n",
      " [58 1 0.0 'center']\n",
      " [22 0 1.0 'suburb']\n",
      " [62 1 0.7 'periphery']\n",
      " [36 0 0.4 'center']\n",
      " [49 1 0.9 'suburb']\n",
      " [55 0 1.5 'periphery']\n",
      " [68 1 0.3 'center']\n",
      " [32 0 0.6 'suburb']\n",
      " [44 1 0.5 'periphery']\n",
      " [80 1 1.0 'suburb']]\n",
      "shape X - rows : 30, columns : 4\n",
      "\n",
      " y : [984 1792 1986 2210 1216 2470 1944 2218 1420 3000 1100 1870 2320 2100 1380\n",
      " 2250 1650 3150 1500 1800 2600 1050 2400 1580 2050 1900 2850 1520 2000\n",
      " 3400]\n",
      "shape y - rows : 30\n"
     ]
    }
   ],
   "source": [
    "# Instanciate function\n",
    "X, y = Separate_X_y(data_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0f668",
   "metadata": {},
   "source": [
    "##### 3.3 Preprocessing categorical variable #####\n",
    "\n",
    "To perform one-hot encoding of the categorical variable, each class is first mapped to a unique integer (0, 1, 2). Then, an identity matrix of size equal to the number of classes is generated. For each observation, the corresponding row of the identity matrix is selected to build the final one-hot encoded matrix of shape (30, 3), where each row contains a single 1 indicating the class of the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "231ae299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping\n",
    "mapping = {\"center\": 0, \"periphery\": 1, \"suburb\" : 2}\n",
    "\n",
    "\n",
    "def mapping_district(X, mapping):\n",
    "    # Apply mapping on \"district\" column\n",
    "    X[:, 3] = np.array([mapping[val] for val in X[:, 3]], dtype = int)\n",
    "    \n",
    "    print(f\"X : {X}\")\n",
    "    print(f\"data type district columns : {X[:, 3].dtype}\")\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "b89326d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[20 0 0.2 0]\n",
      " [35 1 0.1 2]\n",
      " [50 0 0.8 2]\n",
      " [45 1 0.5 1]\n",
      " [28 0 0.3 0]\n",
      " [60 1 1.0 2]\n",
      " [42 0 0.2 1]\n",
      " [55 1 0.4 0]\n",
      " [33 0 1.5 2]\n",
      " [70 1 0.0 1]\n",
      " [25 0 0.1 0]\n",
      " [40 1 0.6 2]\n",
      " [65 0 0.9 2]\n",
      " [48 1 0.3 0]\n",
      " [30 0 0.7 1]\n",
      " [52 1 0.4 2]\n",
      " [38 0 0.2 0]\n",
      " [75 1 1.2 2]\n",
      " [28 1 0.5 1]\n",
      " [46 0 0.8 2]\n",
      " [58 1 0.0 0]\n",
      " [22 0 1.0 2]\n",
      " [62 1 0.7 1]\n",
      " [36 0 0.4 0]\n",
      " [49 1 0.9 2]\n",
      " [55 0 1.5 1]\n",
      " [68 1 0.3 0]\n",
      " [32 0 0.6 2]\n",
      " [44 1 0.5 1]\n",
      " [80 1 1.0 2]]\n",
      "data type district columns : object\n"
     ]
    }
   ],
   "source": [
    "# Instanciate function\n",
    "X = mapping_district(X,mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "817d1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical variable to ease preprocessing\n",
    "X_num = X[:, :-1]\n",
    "X_cat = X[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2e8c2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int since these values will be used as indices \n",
    "# to select the corresponding row from the identity matrix\n",
    "X_cat = X_cat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f107c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2,\n",
       "       1, 0, 2, 1, 0, 2, 1, 2])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f5b11b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 0, 0.2],\n",
       "       [35, 1, 0.1],\n",
       "       [50, 0, 0.8],\n",
       "       [45, 1, 0.5],\n",
       "       [28, 0, 0.3],\n",
       "       [60, 1, 1.0],\n",
       "       [42, 0, 0.2],\n",
       "       [55, 1, 0.4],\n",
       "       [33, 0, 1.5],\n",
       "       [70, 1, 0.0],\n",
       "       [25, 0, 0.1],\n",
       "       [40, 1, 0.6],\n",
       "       [65, 0, 0.9],\n",
       "       [48, 1, 0.3],\n",
       "       [30, 0, 0.7],\n",
       "       [52, 1, 0.4],\n",
       "       [38, 0, 0.2],\n",
       "       [75, 1, 1.2],\n",
       "       [28, 1, 0.5],\n",
       "       [46, 0, 0.8],\n",
       "       [58, 1, 0.0],\n",
       "       [22, 0, 1.0],\n",
       "       [62, 1, 0.7],\n",
       "       [36, 0, 0.4],\n",
       "       [49, 1, 0.9],\n",
       "       [55, 0, 1.5],\n",
       "       [68, 1, 0.3],\n",
       "       [32, 0, 0.6],\n",
       "       [44, 1, 0.5],\n",
       "       [80, 1, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "11e20b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(X_cat):\n",
    "    \"\"\"\n",
    "    One-hot encode a categorical variable using an identity matrix.\n",
    "\n",
    "    Steps:\n",
    "    - Determine the number of classes from X_cat (here 3: 0, 1, 2).\n",
    "    - Create an identity matrix of size (n_classes, n_classes).\n",
    "    - Use the values in X_cat as indices to select the corresponding\n",
    "      rows from the identity matrix, producing the one-hot matrix.\n",
    "    - Drop the last column to avoid perfect multicollinearity \n",
    "      (dummy variable trap).\n",
    "    \"\"\"\n",
    "\n",
    "    n_classes = int(X_cat.max() + 1) # here 3 classes (0,1,2)\n",
    "\n",
    "    # Create an identity matrix of size n_classes and index it with X_cat\n",
    "    local_onehot = np.eye(n_classes)[X_cat]\n",
    "\n",
    "    # Drop the last column to avoid multicollinearity\n",
    "    local_onehot_reduced = local_onehot[:, :-1]\n",
    "\n",
    "    print(f\"Identity Matrx : \\n{np.eye(n_classes)}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"District column onehot : \\n{local_onehot_reduced}\")\n",
    "    \n",
    "    return local_onehot_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "05178fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrx : \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "\n",
      "District column onehot : \n",
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Instanciate function\n",
    "local_onehot_reduced = encode_cat(X_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e3fdb",
   "metadata": {},
   "source": [
    "**Aid to intuition**\n",
    "\n",
    "`Step 1 : Index (class)   One-hot vector`\n",
    "\n",
    "0   ─────────▶   [1 0 0]\n",
    "\n",
    "1   ─────────▶   [0 1 0]\n",
    "\n",
    "2   ─────────▶   [0 0 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13926ae1",
   "metadata": {},
   "source": [
    "`Step 2 : Each row corresponds to a class`\n",
    "\n",
    "Class 0 → 1 in the first position\n",
    "\n",
    "Class 1 → 1 in the second position\n",
    "\n",
    "Class 2 → 1 in the third position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8715b",
   "metadata": {},
   "source": [
    "`Step 3: indexing with np.eye(3)[X_cat]`\n",
    "\n",
    "X_cat[0] = 0 → take row 0 of the table → [1 0 0]\n",
    "\n",
    "X_cat[1] = 2 → take row 2 of the table → [0 0 1]\n",
    "\n",
    "X_cat[2] = 1 → take row 1 of the table → [0 1 0]\n",
    "\n",
    "X_cat[3] = 0 → again row 0 → [1 0 0]\n",
    "\n",
    "...\n",
    "\n",
    "Finally, drop the last column to avoid multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03f190",
   "metadata": {},
   "source": [
    "##### 3.4 Concatenate and add biais #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "35951d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 0, 0.2, 1.0, 0.0],\n",
       "       [35, 1, 0.1, 0.0, 0.0],\n",
       "       [50, 0, 0.8, 0.0, 0.0],\n",
       "       [45, 1, 0.5, 0.0, 1.0],\n",
       "       [28, 0, 0.3, 1.0, 0.0],\n",
       "       [60, 1, 1.0, 0.0, 0.0],\n",
       "       [42, 0, 0.2, 0.0, 1.0],\n",
       "       [55, 1, 0.4, 1.0, 0.0],\n",
       "       [33, 0, 1.5, 0.0, 0.0],\n",
       "       [70, 1, 0.0, 0.0, 1.0],\n",
       "       [25, 0, 0.1, 1.0, 0.0],\n",
       "       [40, 1, 0.6, 0.0, 0.0],\n",
       "       [65, 0, 0.9, 0.0, 0.0],\n",
       "       [48, 1, 0.3, 1.0, 0.0],\n",
       "       [30, 0, 0.7, 0.0, 1.0],\n",
       "       [52, 1, 0.4, 0.0, 0.0],\n",
       "       [38, 0, 0.2, 1.0, 0.0],\n",
       "       [75, 1, 1.2, 0.0, 0.0],\n",
       "       [28, 1, 0.5, 0.0, 1.0],\n",
       "       [46, 0, 0.8, 0.0, 0.0],\n",
       "       [58, 1, 0.0, 1.0, 0.0],\n",
       "       [22, 0, 1.0, 0.0, 0.0],\n",
       "       [62, 1, 0.7, 0.0, 1.0],\n",
       "       [36, 0, 0.4, 1.0, 0.0],\n",
       "       [49, 1, 0.9, 0.0, 0.0],\n",
       "       [55, 0, 1.5, 0.0, 1.0],\n",
       "       [68, 1, 0.3, 1.0, 0.0],\n",
       "       [32, 0, 0.6, 0.0, 0.0],\n",
       "       [44, 1, 0.5, 0.0, 1.0],\n",
       "       [80, 1, 1.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Horizontally concatenate X_num (numerical variables) and one-hot columns\n",
    "X_concat = np.hstack([X_num, local_onehot_reduced])\n",
    "X_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "01537a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add biais (intercept)\n",
    "biais = np.ones((len(X_concat), 1))\n",
    "biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2bca4fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biais.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7df7",
   "metadata": {},
   "source": [
    "**Why the bias (intercept) is important** \n",
    "\n",
    "In a linear model, the prediction is expressed as:  \n",
    "\n",
    "y = β0 + β1·x1 + β2·x2 + ... + βp·xp\n",
    "\n",
    "- **\\(\\beta_0\\) is the bias (intercept)**. It represents the point where the regression line (or hyperplane) crosses the y-axis.  \n",
    "- Without the intercept, the model would be forced to go through the origin (0,0). This is very restrictive and often unrealistic, because in most real-world problems the target variable does not vanish when all features are zero.  \n",
    "- Adding the bias increases the **flexibility** of the model: it allows the line (in 2D) or the hyperplane (in higher dimensions) to shift up or down to better fit the data.  \n",
    "- From a linear algebra perspective, the bias can be seen as adding an **extra feature column of ones** to the design matrix \\(X\\).  \n",
    "- In statistical terms, the intercept captures the **baseline value** of the target when all other explanatory variables are zero.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "18e8d0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 20, 0, 0.2, 1.0, 0.0],\n",
       "       [1.0, 35, 1, 0.1, 0.0, 0.0],\n",
       "       [1.0, 50, 0, 0.8, 0.0, 0.0],\n",
       "       [1.0, 45, 1, 0.5, 0.0, 1.0],\n",
       "       [1.0, 28, 0, 0.3, 1.0, 0.0],\n",
       "       [1.0, 60, 1, 1.0, 0.0, 0.0],\n",
       "       [1.0, 42, 0, 0.2, 0.0, 1.0],\n",
       "       [1.0, 55, 1, 0.4, 1.0, 0.0],\n",
       "       [1.0, 33, 0, 1.5, 0.0, 0.0],\n",
       "       [1.0, 70, 1, 0.0, 0.0, 1.0],\n",
       "       [1.0, 25, 0, 0.1, 1.0, 0.0],\n",
       "       [1.0, 40, 1, 0.6, 0.0, 0.0],\n",
       "       [1.0, 65, 0, 0.9, 0.0, 0.0],\n",
       "       [1.0, 48, 1, 0.3, 1.0, 0.0],\n",
       "       [1.0, 30, 0, 0.7, 0.0, 1.0],\n",
       "       [1.0, 52, 1, 0.4, 0.0, 0.0],\n",
       "       [1.0, 38, 0, 0.2, 1.0, 0.0],\n",
       "       [1.0, 75, 1, 1.2, 0.0, 0.0],\n",
       "       [1.0, 28, 1, 0.5, 0.0, 1.0],\n",
       "       [1.0, 46, 0, 0.8, 0.0, 0.0],\n",
       "       [1.0, 58, 1, 0.0, 1.0, 0.0],\n",
       "       [1.0, 22, 0, 1.0, 0.0, 0.0],\n",
       "       [1.0, 62, 1, 0.7, 0.0, 1.0],\n",
       "       [1.0, 36, 0, 0.4, 1.0, 0.0],\n",
       "       [1.0, 49, 1, 0.9, 0.0, 0.0],\n",
       "       [1.0, 55, 0, 1.5, 0.0, 1.0],\n",
       "       [1.0, 68, 1, 0.3, 1.0, 0.0],\n",
       "       [1.0, 32, 0, 0.6, 0.0, 0.0],\n",
       "       [1.0, 44, 1, 0.5, 0.0, 1.0],\n",
       "       [1.0, 80, 1, 1.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate biais first column\n",
    "X_final = np.hstack([biais, X_concat])\n",
    "X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f01e56",
   "metadata": {},
   "source": [
    "#### 3.4 Train test split ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadd45a",
   "metadata": {},
   "source": [
    "**Why do we use a train/test split?**\n",
    "\n",
    "The idea is simple: we want to check if our model has really **learned something general**, or if it has just **memorized the data**.\n",
    "\n",
    "- **Training set** → the part of the data the model actually sees and learns from.  \n",
    "- **Test set** → completely unseen data, used only at the end to check if the model can make correct predictions on new examples.  \n",
    "\n",
    "If we don’t split, the model could look perfect (because it just repeats what it has already seen), but in reality it would fail on new data.  \n",
    "So the split is essential to evaluate the **true performance** and avoid the trap of **overfitting**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "49a63be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(X_final)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e2b02369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test_Split(X, y, split):\n",
    "\n",
    "    X_train = X[:split, :]\n",
    "    print(f\"X_train : \\n{X_train}\")\n",
    "\n",
    "    X_test = X[split :, :]\n",
    "    print(f\"\\nX_test : \\n{X_test}\")\n",
    "    \n",
    "    y_train = y[:split]\n",
    "    y_test  = y[split:]\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f\"X_train shape :{X_train.shape}, y_train shape : {y_train.shape}\")\n",
    "    print(f\"X_test  shape :{X_test.shape}, y_test  shape : {y_test.shape}\")\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "cee330b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : \n",
      "[[1.0 20 0 0.2 1.0 0.0]\n",
      " [1.0 35 1 0.1 0.0 0.0]\n",
      " [1.0 50 0 0.8 0.0 0.0]\n",
      " [1.0 45 1 0.5 0.0 1.0]\n",
      " [1.0 28 0 0.3 1.0 0.0]\n",
      " [1.0 60 1 1.0 0.0 0.0]\n",
      " [1.0 42 0 0.2 0.0 1.0]\n",
      " [1.0 55 1 0.4 1.0 0.0]\n",
      " [1.0 33 0 1.5 0.0 0.0]\n",
      " [1.0 70 1 0.0 0.0 1.0]\n",
      " [1.0 25 0 0.1 1.0 0.0]\n",
      " [1.0 40 1 0.6 0.0 0.0]\n",
      " [1.0 65 0 0.9 0.0 0.0]\n",
      " [1.0 48 1 0.3 1.0 0.0]\n",
      " [1.0 30 0 0.7 0.0 1.0]\n",
      " [1.0 52 1 0.4 0.0 0.0]\n",
      " [1.0 38 0 0.2 1.0 0.0]\n",
      " [1.0 75 1 1.2 0.0 0.0]\n",
      " [1.0 28 1 0.5 0.0 1.0]\n",
      " [1.0 46 0 0.8 0.0 0.0]\n",
      " [1.0 58 1 0.0 1.0 0.0]\n",
      " [1.0 22 0 1.0 0.0 0.0]\n",
      " [1.0 62 1 0.7 0.0 1.0]\n",
      " [1.0 36 0 0.4 1.0 0.0]]\n",
      "\n",
      "X_test : \n",
      "[[1.0 49 1 0.9 0.0 0.0]\n",
      " [1.0 55 0 1.5 0.0 1.0]\n",
      " [1.0 68 1 0.3 1.0 0.0]\n",
      " [1.0 32 0 0.6 0.0 0.0]\n",
      " [1.0 44 1 0.5 0.0 1.0]\n",
      " [1.0 80 1 1.0 0.0 0.0]]\n",
      "\n",
      "\n",
      "X_train shape :(24, 6), y_train shape : (24,)\n",
      "X_test  shape :(6, 6), y_test  shape : (6,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = Train_Test_Split(X_final, y, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f0e07",
   "metadata": {},
   "source": [
    "#### 3.5 Preprocessing numerical variables (standardisation) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034698",
   "metadata": {},
   "source": [
    "### Why do we use standardization (z-score)?\n",
    "\n",
    "Standardization with the z-score does two main things:\n",
    "\n",
    "- **Avoid bias in the model**: if we feed the model raw numerical data with very different units, one variable can dominate the others.  \n",
    "  Example: a variable measured in thousands of inhabitants will have much more impact than another measured in centimeters.  \n",
    "  By applying z-score standardization \\((X - mean) / std), all features are rescaled to the same range (mean 0, std 1), which makes the training more balanced.\n",
    "\n",
    "- **Spot potential outliers**: after standardization, most values fall within about 3 standard deviations of the mean.  \n",
    "  A point with \\(|z| > 3\\) is likely an outlier, which could bias the model or add noise.\n",
    "\n",
    "In short, standardization makes learning easier for the model (faster and more stable convergence) and also helps detect extreme values that may require special handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87beeb5c",
   "metadata": {},
   "source": [
    "⚠️ Important note about standardization (z-score):\n",
    "\n",
    "When applying z-score standardization, the **mean** and **standard deviation** must be computed **only on the training set (X_train)**.  \n",
    "These same values are then used to transform both **X_train** and **X_test**.\n",
    "\n",
    "Why?  \n",
    "- If we use the mean/std of X_test, we are leaking information from the test set into the training process (**data leakage**).  \n",
    "- The goal is to simulate real-world conditions: when the model sees new data (test or production), it must be standardized with the parameters learned only from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f43346ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train_test(X_train, X_test, cols_to_scale):\n",
    "    \"\"\"\n",
    "    Standardize selected columns of X (z-score) using the statistics computed from the training set.  \n",
    "    Do not modify the bias column (col 0) or any columns not listed.  \n",
    "    Return X_train_std, X_test_std, and (means, stds) for these columns.\n",
    "\n",
    "    \"\"\"\n",
    "    Xtr = X_train.copy().astype(float)\n",
    "    Xte = X_test.copy().astype(float)\n",
    "\n",
    "    means = Xtr[:, cols_to_scale].mean(axis=0)\n",
    "    stds  = Xtr[:, cols_to_scale].std(axis=0)\n",
    "    stds[stds == 0] = 1.0  # éviter division par zéro\n",
    "\n",
    "    Xtr[:, cols_to_scale] = (Xtr[:, cols_to_scale] - means) / stds\n",
    "    Xte[:, cols_to_scale] = (Xte[:, cols_to_scale] - means) / stds\n",
    "    return Xtr, Xte, (means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "96e987ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = [1, 3] # indice col to scale\n",
    "X_train_norm, X_test_norm, _ = standardize_train_test(X_train, X_test, cols_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "338b8c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.59813621,  0.        , -0.86146098,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.61129395,  1.        , -1.11989928,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  0.3755483 ,  0.        ,  0.68916879,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  0.04660088,  1.        , -0.0861461 ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        , -1.07182034,  0.        , -0.60302269,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  1.03344314,  1.        ,  1.20604538,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.15076757,  0.        , -0.86146098,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        ,  0.70449572,  1.        , -0.34458439,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.74287292,  0.        ,  2.49823686,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  1.69133797,  1.        , -1.37833758,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        , -1.26918879,  0.        , -1.11989928,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.28234653,  1.        ,  0.1722922 ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  1.36239056,  0.        ,  0.94760708,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  0.24396933,  1.        , -0.60302269,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.94024137,  0.        ,  0.43073049,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        ,  0.50712727,  1.        , -0.34458439,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.4139255 ,  0.        , -0.86146098,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  2.02028539,  1.        ,  1.72292197,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -1.07182034,  1.        , -0.0861461 ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        ,  0.11239037,  0.        ,  0.68916879,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  0.90186417,  1.        , -1.37833758,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -1.46655724,  0.        ,  1.20604538,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  1.1650221 ,  1.        ,  0.43073049,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        , -0.54550447,  0.        , -0.34458439,  1.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a091b348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.30975882,  1.        ,  0.94760708,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        ,  0.70449572,  0.        ,  2.49823686,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        ,  1.55975901,  1.        , -0.60302269,  1.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.8086624 ,  0.        ,  0.1722922 ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 1.        , -0.0191886 ,  1.        , -0.0861461 ,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.        ,  2.34923281,  1.        ,  1.20604538,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31faaa7e",
   "metadata": {},
   "source": [
    "eeeee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca99b7",
   "metadata": {},
   "source": [
    "#### 3.6 Outliers ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e458457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-score range (train): min = -1.60, max = 2.50\n"
     ]
    }
   ],
   "source": [
    "# Get max absolute value in the scaled column\n",
    "cols_train_z_score = X_train_norm[:, cols_to_scale]\n",
    "max_train_z = cols_train_z_score.max()\n",
    "min_train_z = cols_train_z_score.min()\n",
    "\n",
    "print(f\"Z-score range (train): min = {min_train_z:.2f}, max = {max_train_z:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ffe90363",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_test_z_score = X_test_norm[:, cols_to_scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3ae676c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-score range (train): min = -0.81, max = 2.50\n"
     ]
    }
   ],
   "source": [
    "# Get max absolute value in the scaled column\n",
    "cols_test_z_score = X_test_norm[:, cols_to_scale]\n",
    "max_test_z = cols_test_z_score.max()\n",
    "min_test_z = cols_test_z_score.min()\n",
    "\n",
    "print(f\"Z-score range (train): min = {min_test_z:.2f}, max = {max_test_z:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079a1ee",
   "metadata": {},
   "source": [
    "We observe that there are no points outside the interval [-3z, 3z].  \n",
    "If there had been, they should have been treated as outliers — for example by replacing them with the median, the mean, the nearest neighbor, or by removing the row. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999092ff",
   "metadata": {},
   "source": [
    "### **4. Model training** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf9705",
   "metadata": {},
   "source": [
    "#### 4.1 Weight vector initialization ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a657be0b",
   "metadata": {},
   "source": [
    "Role of the weights and the gradient\n",
    "\n",
    "In linear regression, the weights (β) represent how much each explanatory variable contributes to the prediction.\n",
    "During training, these weights are iteratively updated using gradient descent.\n",
    "\n",
    "The update rule is based on the gradient of the cost function :  (MSE):\n",
    "\n",
    "`mse = ∇β (1/n) Σ (y_i - x_i^T β)^2 = (2/n) X^T (Xβ - y)` # ps: XB = ypred\n",
    "\n",
    "This is the simplified, factorized form of the partial derivatives of the cost function with respect to all parameters, including the intercept.\n",
    "\n",
    "`gradient mse = 2/n*X.T(XB - y)`\n",
    "\n",
    "In short: the sign of the gradient tells us whether to increase or decrease each weight.\n",
    "This is how the model gradually learns the optimal combination of weights to minimize the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a43ef",
   "metadata": {},
   "source": [
    "The gradient indicates the direction of the steepest increase of the MSE. To minimize the error, we move in the opposite direction. The learning rate controls the step size of this update. The gradient is a vector of size X.shape[1], one component per parameter.\n",
    "\n",
    "\n",
    "- `If (∂J/∂β_j) > 0` \n",
    "  → increasing β_j increases the loss  \n",
    "  → so we need to decrease β_j.\n",
    "\n",
    "- `If (∂J/∂β_j) < 0`  \n",
    "  → increasing β_j decreases the loss  \n",
    "  → so we need to increase β_j.\n",
    "\n",
    "  β_j is the weight (or coefficient) applied to the variable x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "63d9d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta shape: (6,)\n",
      "Initialized beta vector: [ 0.00304717 -0.01039984  0.00750451  0.00940565 -0.01951035 -0.0130218 ]\n"
     ]
    }
   ],
   "source": [
    "# Weight initialization\n",
    "# Since the MSE loss function is convex (only one global minimum),\n",
    "# there is no risk of getting stuck in a local minimum.\n",
    "# Therefore, a simple initialization can be used.\n",
    "\n",
    "# Assume X_train is already defined\n",
    "n_features = X_train_norm.shape[1]   # number of columns (features + bias)\n",
    "\n",
    "# Random generator (seed for reproducibility)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Initialization: random values ~ N(0, 0.01) -> Normal law\n",
    "beta_init = rng.normal(loc=0.0, scale=0.01, size=n_features)\n",
    "\n",
    "print(\"Beta shape:\", beta_init.shape)\n",
    "print(\"Initialized beta vector:\", beta_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bee86",
   "metadata": {},
   "source": [
    "#### 4.2 Sanity Check ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "56a31c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All shape checks passed. Dimensions are consistent.\n"
     ]
    }
   ],
   "source": [
    "assert X_train.shape[1] == X_test.shape[1], \"Train/Test do not have the same number of columns\"\n",
    "assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train do not have the same number of samples\"\n",
    "assert X_test.shape[0]  == y_test.shape[0],  \"X_test and y_test do not have the same number of samples\"\n",
    "assert beta_init.shape[0] == X_train.shape[1], \"The weight vector and X_train do not have the same number of columns\"\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # if (n,1) -> y = y.ravel()\n",
    "print(\"✅ All shape checks passed. Dimensions are consistent.\")\n",
    "\n",
    "# Ensure the data is in float type for linear algebra\n",
    "X_train = X_train.astype(float)\n",
    "X_test  = X_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test  = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d95fe0",
   "metadata": {},
   "source": [
    "#### 4.3 Gradient Descent Algorithm ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909883ce",
   "metadata": {},
   "source": [
    "**Regularization L2 : λ**\n",
    "\n",
    "The goal of regularization is to reduce overfitting by penalizing large weights. It keeps the model simpler, more stable, and improves generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0f8e4",
   "metadata": {},
   "source": [
    "### Cost functions (MSE)\n",
    "\n",
    "- **MSE only (no Ridge):**  \n",
    "`J(β) = (1/n) · Σ (yᵢ - xᵢᵀβ)²`  \n",
    "\n",
    "- **MSE + Ridge (L2):**  \n",
    "`J(β) = (1/n) · Σ (yᵢ - xᵢᵀβ)²  +  λ · Σ βⱼ²`\n",
    "(with β₀, the intercept, excluded from regularization)\n",
    "\n",
    "\n",
    "### Gradients (partial derivative)\n",
    "\n",
    "- **MSE only (no Ridge):**  \n",
    "`grad = (2/n) · Xᵀ (Xβ - y)`\n",
    "\n",
    "- **MSE + Ridge (L2):**  \n",
    "`grad = (2/n) · Xᵀ (Xβ - y) + 2λβ`  \n",
    "(with β₀ excluded from regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb48f23",
   "metadata": {},
   "source": [
    "### Ridge intuition  \n",
    "\n",
    "With Ridge, the gradient of each weight **βⱼ** (except the intercept) is updated with an extra term:  \n",
    "\n",
    "`grad[j] += 2λβⱼ`  \n",
    "\n",
    "This pushes every weight back toward zero.  \n",
    "\n",
    "**Example for a single weight from the β vector (beta):**  \n",
    "\n",
    "`λ = 0.1, η = 0.05`\n",
    "Current weight: `β = 2`  \n",
    "Gradient from MSE part only: **grad = +0.6** => **reducing β decrease MSE** \n",
    "\n",
    "- Without Ridge: `β ← 2 − 0.05 · 0.6 = 1.97`  \n",
    "  \n",
    "- With Ridge (extra 2λβ = 0.4): \n",
    "    - β ← β - η · grad \n",
    "    - `β ← 2 − 0.05 · (0.6 + 0.4) = 1.95`  \n",
    "\n",
    "Ridge makes **β shrink faster toward 0**, even if the MSE gradient alone would have moved it less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b34dbe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    X_train, y_train, X_test, y_test, beta_init,\n",
    "    lr=0.05, epochs=500, tol=1e-6,\n",
    "    l2=0.1, average_l2=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Batch gradient descent for linear regression (MSE) with optional Ridge regularization.\n",
    "    Tracks both train and test MSE at each epoch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array (n, p)  -- column 0 = 1 for the intercept\n",
    "    y_train : array (n,)\n",
    "    X_test : array (m, p)\n",
    "    y_test : array (m,)\n",
    "    beta_init : array (p,)\n",
    "    lr : float        -- learning rate\n",
    "    epochs : int\n",
    "    tol : float       -- early stopping if ΔMSE < tol (on train loss)\n",
    "    l2 : float        -- Ridge coefficient (β0 not penalized)\n",
    "    average_l2 : bool -- if True, penalty / n (consistent with averaged MSE)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : (p,)\n",
    "    history_train : (t,) -- train MSE per iteration\n",
    "    history_test : (t,)  -- test MSE per iteration\n",
    "    stopped_at : int     -- number of updates performed (1..epochs)\n",
    "    last_loss : float    -- final train MSE\n",
    "    grad : (p,)          -- last gradient\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    n, _ = X_train.shape\n",
    "    beta = np.asarray(beta_init, dtype=float).copy()\n",
    "\n",
    "    history_train = []\n",
    "    history_test = []\n",
    "    stopped_at = epochs\n",
    "\n",
    "    for t in range(epochs):\n",
    "        # 1) prediction (train)\n",
    "        y_pred_train = X_train @ beta\n",
    "\n",
    "        # 2) train MSE\n",
    "        loss_train = np.mean((y_train - y_pred_train) ** 2)\n",
    "        history_train.append(loss_train)\n",
    "\n",
    "        # 3) gradient (average)\n",
    "        grad = (2.0 / n) * (X_train.T @ (y_pred_train - y_train))\n",
    "\n",
    "        # 4) Ridge regularization (β0 not penalized)\n",
    "        if l2 > 0.0:\n",
    "            reg = beta.copy()\n",
    "            reg[0] = 0.0\n",
    "            scale = (1.0 / n) if average_l2 else 1.0\n",
    "            grad += 2.0 * l2 * scale * reg\n",
    "\n",
    "        # 5) update\n",
    "        beta -= lr * grad\n",
    "\n",
    "        # 6) test MSE\n",
    "        y_pred_test = X_test @ beta\n",
    "        loss_test = np.mean((y_test - y_pred_test) ** 2)\n",
    "        history_test.append(loss_test)\n",
    "\n",
    "        # 7) early stopping (train loss)\n",
    "        if t > 0 and abs(history_train[-2] - history_train[-1]) < tol:\n",
    "            stopped_at = t + 1\n",
    "            break\n",
    "\n",
    "    return beta, np.array(history_train), np.array(history_test), stopped_at, history_train[-1], grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "58ecaddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs performed (updates): 2873\n",
      "\n",
      "\n",
      "Final MSE train: 8124.67020498302\n",
      "\n",
      "\n",
      "Loss history train (last 10 values): [4000956.29829558 2913020.36402547 2127611.74551522 1560250.40467731\n",
      " 1150102.55956548  853351.08927699  638426.91828557  482578.50479964\n",
      "  369403.5741409   287072.89482262]\n",
      "\n",
      "\n",
      "Loss history test (last 10 values): [4113532.23839485 3009112.48570886 2207030.72605473 1624569.03501336\n",
      " 1201666.86716164  894702.58266883  671985.46632409  510486.71020072\n",
      "  393468.08393901  308761.22108563]\n"
     ]
    }
   ],
   "source": [
    "beta_final, hist_train, hist_test, stopped_at, last_loss, grad = gradient_descent(\n",
    "    X_train_norm, y_train,\n",
    "    X_test_norm, y_test,\n",
    "    beta_init,\n",
    "    lr=0.05, epochs=3000, tol=1e-10,\n",
    "    l2=0.0, average_l2=True\n",
    ")\n",
    "\n",
    "print(\"Epochs performed (updates):\", stopped_at)\n",
    "print(\"\\n\")\n",
    "print(\"Final MSE train:\", last_loss)\n",
    "print(\"\\n\")\n",
    "print(\"Loss history train (last 10 values):\", hist_train[:10])\n",
    "print(\"\\n\")\n",
    "print(\"Loss history test (last 10 values):\", hist_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6a998",
   "metadata": {},
   "source": [
    "We see that from epoch 2873 onward, the MSE no longer decreases beyond the minimum threshold of 1e-10 between epochs, so the algorithm stops to prevent overfitting. The mean squared error is 8124, which corresponds to about 90 euros error per prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350e5b4",
   "metadata": {},
   "source": [
    "### **5. Predicitons** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ad5fd",
   "metadata": {},
   "source": [
    "We compute the predictions by multiplying the weight vector learned during training (β) with the feature matrices X_train_norm and X_test_norm.\n",
    "\n",
    "- y_pred_train are the predictions on the training set, i.e. on data the model has already seen. They tell us how well the model fits the known data.\n",
    "\n",
    "- y_pred_test are the predictions on the test set, i.e. on unseen data. This shows the model’s ability to generalize to new inputs.\n",
    "\n",
    "=> That’s the main purpose of the train/test split: check not only if the model learns the training examples, but also if it can make reliable predictions on data it never had access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d351e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1829.32064952,  498.5647684 ,  202.32641613,  -45.33505958,\n",
       "        -68.88526249,   34.9115409 ])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "dbe6136f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -1.59813621,  0.        , -0.86146098,  1.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e343f",
   "metadata": {},
   "source": [
    "**Prediction calculation for the first row**\n",
    "\n",
    "Products detail :\n",
    "\n",
    "| Feature (X)   | Coefficient (β)   | Product (X·β)  |\n",
    "|---------------|-------------------|----------------|\n",
    "| 1             | 1829.32064952     | 1829.32        |\n",
    "| -1.5981       | 498.5647684       | ≈ -796.3       |\n",
    "| 0             | 202.32641613      | 0              |\n",
    "| -0.8615       | -45.33505958      | ≈ +39.0        |\n",
    "| 1             | -68.88526249      | -68.89         |\n",
    "| 0             | 34.9115409        | 0              |\n",
    "\n",
    "Sum :\n",
    "\n",
    "1829.32 − 796.3 + 0 + 39.0 − 68.9 + 0 ≈ **1003.1**\n",
    "\n",
    "Final prediction for the first row: **ŷ₀ ≈ 1003.1** (corresponding to first value of y_pred_train vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8878198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1002.71536513 1777.64813886 1985.31229371 2093.69760409 1253.40159873\n",
      " 2492.2092652  1828.11917859 2329.62030261 1345.69266821 2972.28714766\n",
      " 1178.43327384 1883.06815447 2465.60075778 2111.73438765 1375.93377733\n",
      " 2300.10460918 1593.12110064 2960.78141376 1536.09218721 1854.11101915\n",
      " 2474.88652059 1043.47074076 2627.87038993 1504.08783233]\n",
      "\n",
      "\n",
      "[2143.12207568 2102.21121919 2767.74076044 1418.33918923 2060.89728545\n",
      " 3148.21563799]\n"
     ]
    }
   ],
   "source": [
    "# Train predictions\n",
    "y_pred_train = X_train_norm @ beta_final\n",
    "print(f\"{y_pred_train}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "# Test Predictions\n",
    "y_pred_test = X_test_norm @ beta_final\n",
    "\n",
    "print(f\"{y_pred_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "53d0dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2050. 1900. 2850. 1520. 2000. 3400.]\n",
      "\n",
      "\n",
      "[2143.12207568 2102.21121919 2767.74076044 1418.33918923 2060.89728545\n",
      " 3148.21563799]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(\"\\n\")\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555897d",
   "metadata": {},
   "source": [
    "### **6. Metrics** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cd69",
   "metadata": {},
   "source": [
    "#### 6.1 MSE (Mean square error) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0e379",
   "metadata": {},
   "source": [
    "As seen earlier, the MSE is the mean of the squared differences between y and the predictions. We compute it both on the training and the test set. It represents the average error per prediction. The error is squared to remove the sign (+/−) and measure only the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f5ad8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "mse_test  = np.mean((y_test  - y_pred_test)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604ae08",
   "metadata": {},
   "source": [
    "#### 6.2 R²-score ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d08bb",
   "metadata": {},
   "source": [
    "We can think of the R² score as a contest between our model and the mean: it shows how much better the model predicts compared to always using the average. The closer R² is to 1, the more accurate the model is. On the other hand, a value around or below 0.5 means the model is not doing better than the mean, or even worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9c4c",
   "metadata": {},
   "source": [
    "`R² = 1 − (Σ(yᵢ − ŷᵢ)²) / (Σ(yᵢ − ȳ)²)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c00715",
   "metadata": {},
   "source": [
    "**Intuition :**\n",
    "\n",
    "- The smaller the numerator (predictions close to y), the smaller the ratio, and the closer R² gets to 1.\n",
    "\n",
    "- If the numerator is about the same as the denominator → R² ≈ 0 (the model is no better than just predicting the mean).\n",
    "\n",
    "- If the numerator is larger than the denominator → R² < 0 (the model is worse than the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9a46a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores :\n",
      "Train -> MSE=8124.670205 | R²=0.975299\n",
      "Test  -> MSE=22294.407569  | R²=0.945005\n"
     ]
    }
   ],
   "source": [
    "def r2(y, y_pred):\n",
    "    ss_res = np.sum((y - y_pred)**2)\n",
    "    ss_tot = np.sum((y - y.mean())**2)\n",
    "    return 1.0 if ss_tot == 0 else 1 - ss_res/ss_tot\n",
    "\n",
    "r2_train = r2(y_train, y_pred_train)\n",
    "r2_test  = r2(y_test,  y_pred_test)\n",
    "\n",
    "print(\"Scores :\")\n",
    "print(f\"Train -> MSE={mse_train:.6f} | R²={r2_train:.6f}\")\n",
    "print(f\"Test  -> MSE={mse_test:.6f}  | R²={r2_test:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91058697",
   "metadata": {},
   "source": [
    "It’s crucial that the training and test scores are roughly similar. If the gap between them is too large, it means the model has overfit the training data (the data it has already seen) and loses its ability to generalize to unseen data (the test set).\n",
    "\n",
    "Here the values are quite similar, so we can consider our model good at generalizin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28312f",
   "metadata": {},
   "source": [
    "#### 6.3 PLOT ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cff58d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist_train[:100], label=\"Train loss (MSE)\")\n",
    "plt.plot(hist_test[:100], label=\"Test loss (MSE)\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Train vs Test Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425b8fe",
   "metadata": {},
   "source": [
    "Here we only plotted the first 100 epochs so that the convergence of the model is easier to see on the graph. As the epochs increase, both the train and test losses decrease and almost overlap, which shows that the model is well fitted without clear signs of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0ce5a",
   "metadata": {},
   "source": [
    "## **7. Using the model on new data** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e366b",
   "metadata": {},
   "source": [
    "**New data :**\n",
    "\n",
    "- surface = 30 m²\n",
    "\n",
    "- furnished = 1\n",
    "\n",
    "- distance_km = 0.3\n",
    "\n",
    "- quartier = center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "60602220",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = [1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d92055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.29166667  0.53333333]\n",
      "[15.19999772  0.38693956]\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "X_train_std, X_test_std, (means, stds) = standardize_train_test(X_train, X_test, cols_to_scale)\n",
    "\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "fd1c4f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.94024137  1.         -0.60302269  1.          0.        ]\n",
      "\n",
      "\n",
      "1521.3286521328566\n"
     ]
    }
   ],
   "source": [
    "# 1. Normalize continuous columns in the raw input\n",
    "x_new_raw = np.array([30.0, 1.0, 0.3, 1.0, 0.0])   # [area, furnished, distance, center, suburb]\n",
    "x_new_scaled = x_new_raw.copy().astype(float)\n",
    "x_new_scaled[[0, 2]] = (x_new_scaled[[0, 2]] - means) / stds   # normalize area and distance\n",
    "\n",
    "# 2. Add the bias term in column 0\n",
    "x_new = np.hstack([1.0, x_new_scaled])\n",
    "\n",
    "# 3. Prediction\n",
    "y_pred = x_new @ beta_final\n",
    "\n",
    "print(x_new)\n",
    "print(\"\\n\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d493b",
   "metadata": {},
   "source": [
    "For a 30 m² furnished apartment located in the city center, 0.3 km from the reference point, the predicted rent is about 1521 €."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d1eb6",
   "metadata": {},
   "source": [
    "### Thank you for reading !!!\n",
    "\n",
    "Thanks for going through this notebook!  \n",
    "I hope it helped you understand how gradient descent and regularization work when building a machine learning model from scratch with NumPy.  \n",
    "\n",
    "There are of course many other aspects to explore. For example:  \n",
    "- **L1 regularization (Lasso)** can shrink some coefficients exactly to zero, which is useful for feature selection.  \n",
    "- **Grid Search** is a systematic way to test multiple hyperparameter values (learning rate, λ, etc.) and find the best configuration.  \n",
    "- **k-fold cross validation** splits the data into *k* parts, trains *k* models on different folds, and averages the results. This reduces the risk of depending too much on a single train/test split and gives a more reliable estimate of model performance.  \n",
    "\n",
    "Happy coding and keep exploring ML ! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
